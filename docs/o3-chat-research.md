Great. I’ll look into high-quality open-source chat solutions and libraries that can accelerate building a Telegram-like messenger system for your Common Ground architecture.

I’ll focus on identifying:

1. Open-source full-stack chat apps using Next.js + Node.js + PostgreSQL + Socket.IO.
2. React component libraries with Telegram-style UX, threading, media/file support, reactions, mobile design, compatible with Tailwind and Next.js 14+.
3. Backend messaging infra with advanced real-time features (typing indicators, receipts, queueing, persistence in PostgreSQL).
4. iframe-embeddable systems supporting standalone and embedded dual-mode deployment.
5. Proven PostgreSQL schemas for chat/conversations/messages that will coexist with your existing schema.
6. Strategies/examples for integrating OpenAI bots as message participants.

I’ll get back to you with repos, component libraries, backend infrastructure picks, schema suggestions, and architectural notes specific to your codebase and system.


# Telegram-Like Messenger for the Common Ground Ecosystem

## Open-Source Chat Solutions (Next.js + Node + PostgreSQL)

To accelerate development, we can evaluate **open-source chat applications** that closely match our tech stack. A standout example is **xTeam Chat** by Aleksander Palamar, a Slack/Discord-like app built with Next.js 13, Socket.IO, Prisma ORM and MySQL (easily switched to PostgreSQL via Prisma). It offers real-time messaging, file attachments (via UploadThing), message editing/deletion, **1:1 DMs, group channels**, emoji reactions, and even voice/video calls. The UI is built with Tailwind + Shadcn (Radix) components, ensuring a responsive Telegram-like feel. This project is MIT-licensed and could serve as a foundation – we’d replace its Clerk auth with our wallet-based auth and adapt schema to our DB. Its architecture (Next.js App Router and a Node Socket.IO server) aligns well with our setup. Another simpler example is *drew18moore’s Realtime Chat*, a Next.js + Express + Socket.IO app with JWT auth and direct messaging. It’s more minimal (DMs only, basic features) but demonstrates the fundamentals of Node/Postgres chat.

For a fully mature solution, we could also consider **Rocket.Chat** – an open-source Slack alternative (Node/Mongo) – or **Mattermost** (Go/Postgres). These are proven in production for group chat, threads, etc., but they have very different architectures. Adapting Rocket.Chat to our stack might be heavy, but it’s worth noting Rocket.Chat supports iframe embedding and custom auth integration (more on that below). In summary, **forking a Next.js-based chat repo** like xTeam Chat is likely the fastest path: we’d gain a complete UI and core logic, then integrate it with our user system. The code appears compatible with our context (it uses Prisma and Tailwind, which we already employ). Its feature set (threads, reactions, invites, etc.) covers our requirements, and we can disable or enable parts as needed.

**Key candidate repos to evaluate:**

* **xTeam Chat (Next.js 13 + Socket.IO)** – Full-featured team chat (DMs, channels, media). We can adapt it to use our `users` table and Socket.IO backend.
* **Drew Moore’s Realtime Chat (React + Node + PG)** – Simple chat with JWT auth. Good reference for basic DM data flow with PostgreSQL.
* **anav5704/scalable-chat** – Example of a horizontally scalable chat using Next.js, Node, Redis, Kafka, Postgres. This shows patterns for high throughput (Kafka message queue) and can inform our architecture if we anticipate scaling needs beyond a single server.

Each of these being open source means we can study and borrow components or even fork directly to jump-start our messenger service.

## React Chat UI Libraries for a Telegram-Like Interface

Instead of building the chat UI from scratch, we should leverage a **React chat UI toolkit** that provides messaging components (message list, bubbles, composer, etc.) similar to Telegram’s interface. One excellent option is the **Chat UI Kit by chatscope**. It’s an open-source library with *43 reusable React components*, fully responsive layouts, and highly customizable styling. It supports theming (400+ CSS variables) and is framework-agnostic, so we can integrate it with Tailwind or our own styles easily. Using this kit, we can assemble chat windows, conversation lists, message input boxes, etc., with minimal effort. Notably, it supports typical features like attachments, emoji, and is accessible (a11y compliant) out-of-the-box. This would cover the “Telegram-like” UI elements (bubbles, scrollable chat history, responsive design) and save us from writing these components manually.

For **threading and reactions**, the chat UI kit can be extended – e.g., by rendering a message with a “replies” button that opens a sub-thread (we can implement this using a modal or nested MessageList). Similarly, reactions can be shown as small counters on messages (the kit doesn’t provide reactions UI by default, but we can integrate an emoji picker and use small badge components for counts). Another library to consider is **assistant-ui** (by Y Combinator’s team), which specifically targets AI chat UX. It provides composable primitives for chat threads with streaming text, code blocks, file uploads, etc., and is built with Shadcn/UI (which matches our design approach). This could be very useful for the **bot integration** aspect: it natively handles streaming responses, “typing” animations, and markdown rendering. We could use assistant-ui for the AI bot conversations UI, ensuring a smooth experience when bots send multi-part or formatted messages.

Aside from these, there are other open-source chat UI libraries like **React Chat Elements** and community-driven projects. However, chatscope’s kit is notably robust and general-purpose. It can be used for everything from 1:1 chat windows to group chat feeds, and even embedded live-chat widgets. It’s also actively maintained (with Storybook docs for each component). Using such a library will ensure our **modal overlay** for chat (similar to our WhatsNew modal) has polished UI components. For example, we can create a `<ChatModal>` component using chatscope’s `<MainContainer><ChatContainer>…</ChatContainer></MainContainer>` layout and mount it similarly to `WhatsNewModal`. This aligns with our existing modal overlay pattern in Curia. Overall, adopting a UI kit means we focus on integration and logic rather than pixel-perfect UI details.

## Backend Infrastructure for Real-Time Chat (Beyond Basic Socket.IO)

Our current real-time features use Socket.IO, which we can extend for messaging. To go **beyond basic Socket.IO**, we should address scalability and delivery guarantees:

* **Scaling Socket.IO:** In a production chat system, if we run multiple Node instances, messages need to broadcast to all servers. Socket.IO offers a Redis adapter for pub/sub – we can plug this in so that when a user sends a message on one server, the Redis channel broadcasts it to subscribers on others. This pattern is standard for scaling WebSocket servers horizontally. The open-source *scalable-chat* app demonstrates this by using Redis to publish messages to all clients in real-time. We can use a similar Redis-backed adapter (e.g., `socket.io-redis`) in our Node service.

* **Message Persistence & Queuing:** Rather than writing to PostgreSQL directly on every message in the WebSocket loop (which can become a bottleneck under load), consider a **queue or stream**. The scalable-chat project uses **Kafka** as a message queue: Socket.IO emits to Redis for realtime, and simultaneously enqueues the message to Kafka. A consumer then writes messages to Postgres asynchronously. This decouples realtime delivery from DB writes, improving throughput and resilience (if DB is slow, the queue buffers the messages). We could adopt a lighter-weight approach (e.g. using **RabbitMQ or BullMQ** with Redis) if Kafka is too complex. But the principle is the same – a background worker handles inserts so the WebSocket server remains responsive. This also opens the door for implementing **delivery acknowledgements** or retries: e.g., if a message fails to save, the consumer can retry without affecting the live chat flow.

* **Typing Indicators & Read Receipts:** These are ephemeral events suited to in-memory tracking. We can use Socket.IO rooms to broadcast “user X is typing” to others in the conversation. No need to persist these. Read receipts (who read up to which message) can be maintained in a small Redis cache or a lightweight table. A common approach is to store a `last_read_message_id` per user per conversation (which we can add to a `conversation_participants` table or a separate `read_receipts` table). Whenever a user opens a chat, the client sends an acknowledgment event for the latest message, which we persist and also broadcast as a “seen” status to others.

* **Bot Messages and Streaming:** For AI agent integration, our backend should support *streaming* responses. Socket.IO can stream by emitting partial message events (e.g., chunks of an AI reply). We might integrate our existing AI conversation logic here. One approach: when a user message is identified as addressed to a bot/agent, the backend can call our AI service (perhaps using the `ai_conversations` and `ai_messages` tables we have) and **stream the assistant’s reply token-by-token** via Socket.IO. Libraries like OpenAI’s Node SDK allow streaming, or we can use our existing mechanism but hook it into the WebSocket. The **assistant-ui** library on the frontend will automatically handle the display of streaming text, so coupling it with Socket.IO streaming will create a smooth bot experience in the chat.

* **Using Our Existing Stack:** We will leverage our existing Node/Express (Next.js API routes or a separate Node server). Next.js 14 can run Edge middleware or Server Actions, but for persistent WebSocket, we’ll likely run a custom server (or a Next API route that upgrades to a socket). We already have Socket.IO set up (likely in Curia’s real-time features), so we can either extend that server to handle chat events, or start a new Socket.IO service in the chat module. Given our **common Redis cache** (we use Redis for caching presence or session data), plugging Socket.IO’s adapter will be straightforward.

In summary, **the backend will consist of**: Socket.IO for realtime bi-directional communication, Redis for scaling and possibly caching user presence, PostgreSQL for storing messages, and optional message queueing (Kafka/RabbitMQ) for high throughput persistence. This ensures real-time delivery with reliability. For example, WhatsApp’s system (as noted in an expert discussion) uses a socket connection for live delivery and a store-and-forward mechanism for offline users. We can implement a simplified version: if a user is offline, we mark their messages undelivered in DB, and when they reconnect, send any missed messages from the DB. Our `authentication_sessions` table can help track online status (active sessions). Using these patterns will give us a robust infrastructure beyond a naive “send and pray” Socket.IO setup.

## Integration Architecture – Standalone vs. Embedded Module

One **critical requirement** is that this messenger operates both as a standalone service *and* as an integrated module in Curia. We should follow a similar approach to our existing plugin system (as seen in `host-service`). Specifically, the chat app will be a Next.js application that can detect whether it’s running **inside an iframe** (embedded in the forum) or on its own domain. We already have patterns for this in our code: for example, `SidebarActionListener` in Curia listens for a postMessage event of type `'sidebar_action'` to open certain UIs. Currently, clicking the “Messages” icon in the sidebar would trigger `MessageRouter.sendSidebarAction('messages')` on the host side, and our forum app is set up to receive that (there’s a TODO to implement the messages interface on receipt of this action). We will implement this such that, when the forum receives the `'messages'` command, it **mounts the chat UI**.

**How to mount the chat UI?** There are two approaches:

1. *Iframe Embedding:* We load the chat app in an iframe (e.g., an overlay dialog containing an `<iframe src="https://chat.domain/?iframeUid=...">`). The host and chat iframe then communicate via the **MessageRouter** (postMessage API). This is analogous to how our host-service loads external plugins. In fact, Rocket.Chat uses a very similar integration model: you can embed a Rocket.Chat channel via iframe and pass authentication from the parent site, giving a seamless experience without separate login. Rocket.Chat’s iframe integration allows the parent to handle user login and then trust the iframe via a token, and also supports event messaging (parent <-> chat commands). We can emulate this: when Curia loads the chat iframe, it can generate a one-time **session token** for the user (perhaps a JWT or a random token saved in `authentication_sessions`) and include it in the iframe URL or send it via postMessage once the iframe is loaded. The chat app would receive that token and use it to auth the user in its context (e.g., the iframe could make an `/api/auth` call to Curia backend with the token to establish a session). This **single sign-on via iframe** means the user doesn’t have to log in again inside the chat module. The iframe can also listen to events like “user logged out” or “new friend added” from the parent if needed, though most heavy communication can be done via shared backend (since both iframe and parent can query the same database).

2. *Direct React Component (no iframe):* Alternatively, we integrate the chat UI directly into Curia’s codebase (e.g., as a route or modal component), but make it capable of running standalone. This would mean our chat code lives in the mono-repo and can be deployed separately. However, given the requirement of a **separate service**, the iframe route is cleaner. It enforces a separation of concerns (the chat service could even be deployed independently and just embedded when needed). It also matches our “dual-mode plugin” concept – much like how the Common Ground plugin instance works in `CgLibContext`, where the app either uses a provided `iframeUid` (when embedded) or redirects to the standalone `PLUGIN_INSTANCE_URL` if accessed directly. We can apply the same pattern: if someone visits the chat app directly in a browser, it can either function as a standalone (showing its own login page for wallet auth) or even redirect to a known host page. In embedded mode, the chat app will receive an `iframeUid` or similar identifier from the parent context (as our `CgPluginLib.initialize` currently does).

Practically, we will **use the host-service’s MessageRouter**. For example, the parent (Curia forum) might include a placeholder `<div id="chat-container" />` for the chat iframe or modal. When the SidebarActionListener catches the `'messages'` action, we inject or reveal the chat component. If using an iframe, we create it pointing to our chat service URL. Then, using `window.postMessage`, we can pass initial context (like the user’s session token or user ID). Our MessageRouter already supports sending events to iframes and receiving responses (we extend InternalMessageType with custom actions). We will add handlers so that, for example, the chat iframe can notify the parent of certain events – e.g., if the user receives a new message while the chat is open, we might want to update a badge in the parent UI. Conversely, if the parent needs to tell the chat about something (like “user profile updated”), we could send a command. This loosely mirrors Rocket.Chat’s *send/receive commands* feature for embedded mode.

For **standalone mode**, the chat app would have its own entry (its own Next.js pages). We’d implement wallet login here by reusing our `curia/src/lib/auth` logic. That likely means exposing an API route in the chat service that mirrors Curia’s `/api/auth` endpoints (or, if we point the chat service at the same database, the login could even be done by calling Curia’s API – but ideally the chat service can operate independently). Upon login, it would create a session in the shared `authentication_sessions` table (so that sessions are recognized across services if on the same domain, or separate if not). One idea is to have the chat service share the top-level domain (e.g., forum at `example.com`, chat at `chat.example.com`) so that we could potentially share cookies if needed. If not, a more decoupled approach is fine – standalone just means the chat has full auth and user management capabilities on its own.

**Integration Example:** A user clicks the “Messages” icon in Curia. The host’s `MessageRouter.sendSidebarAction('messages')` is invoked. Curia’s `SidebarActionListener` receives this and, say, opens a `<ChatModal>` that contains the chat iframe. The chat iframe loads, and on load it sends an init message to parent like `{ type: 'init_request' }`. The parent responds with `{ type: 'init_response', token: <JWT>, userId: <id> }` via MessageRouter. The iframe then uses this info to authenticate (perhaps storing the token and proceeding to load conversations via its API). From the user’s perspective, it looks like an integrated part of the forum UI, but under the hood it’s an isolated service communicating through a secure channel. This design fulfills the **dual deployment** requirement: the same chat app could be deployed at `chat.example.com` (for standalone) and also be reachable in an embedded context within the forum.

By following this architecture, we leverage our existing cross-iframe messaging system and avoid tightly coupling the chat into the forum codebase. It remains a **pluggable module**. If needed, other platforms could embed our messenger in the future using a similar iframe + token approach. (This is analogous to how Discourse forum’s chat plugin works – though internal to Discourse – or how Intercom/Chatwoot widgets embed on websites). The important part is managing authentication and data sharing cleanly across the boundary, which the above token exchange accomplishes.

## Database Schema and Migration Strategy for Chat

We will introduce new tables for the chat system, designed to **coexist with the current schema** without conflicts. Based on common models and our requirements, the following schema is recommended:

* **conversations** – Represents a chat thread (either a 1:1 DM or a group chat). Key fields: `id` (primary key, e.g. UUID or serial), `is_group` (boolean) or a `type` enum, optional `title` (for group chat name), optional `community_id` (if we tie group chats to a specific community context), `created_at`. If we want to support cross-community or global chats, we might leave `community_id` null for DMs or non-community groups. This table will link to our existing **communities** table via `community_id` (with foreign key) if used, but that won’t conflict – it’s just additional data linking.

* **conversation\_participants** – A join table between `conversations` and `users`. Each row indicates a user is a member of a conversation. Fields: `conversation_id` (FK to conversations), `user_id` (FK to users.user\_id), and perhaps role or status (e.g., in group chats, could mark an admin, or if they left the chat). This table allows any number of users per conversation (so it covers both DMs and groups). For a DM, there will simply be two entries for the two users. For a group of N users, N entries. Using this many-to-many schema is the flexible approach, confirmed by database design best practices. It replaces older approaches of having fixed “user1, user2” columns which don’t scale to groups.

* **messages** – Stores individual chat messages. Fields: `id` (primary key, could be UUID or bigserial), `conversation_id` (FK to conversations), `sender_user_id` (FK to users), `content` (text or JSON for message content; could include message text, and maybe a JSON blob for attachments or metadata), `created_at` timestamp. We might also include `reply_to_message_id` if we want to model threaded replies (this would be analogous to a “thread ID” linking to another message in the same conversation). Each message row thus links to a conversation and a sender. This is similar to how our forum posts/comments are structured, but optimized for chat (lighter weight content). An expert database commenter suggests exactly this setup: a separate messages table with conversation ID, user ID, timestamp, and content.

* **attachments** (optional) – If we want to handle file uploads in a structured way, we can have an attachments table: `id`, `message_id` (FK to messages), file URL or path, file type, etc. Alternatively, we might store attachment info in message.content as JSON (for example, content could have a type “image” and a URL). Depending on complexity, a separate table is cleaner for large files.

* **message\_reactions** (optional) – To support per-message emoji reactions. We can mirror our `reactions` table structure, but add `message_id` as a foreign key. In fact, we might choose to **extend our existing `reactions` table**: currently it supports reactions for posts, comments, and locks via polymorphic columns. We could add `message_id` as another nullable field and update the constraint (so a reaction is tied to exactly one of post/comment/lock/message). This way we reuse the table for all reaction data. This requires a schema migration to add the column and adjust check constraints.

* **read\_receipts** (optional but likely needed for UX) – A small table or an extension of participants table to track read status. Simplest approach: add a column `last_read_message_id` or `last_read_at` in **conversation\_participants**. Each time a user reads new messages, update this. Then we can compute unread counts easily by comparing with latest message ID in the conversation. This avoids needing a separate table for read receipts. If per-message read receipts (who saw a specific message) are desired, that’s more granular and could be another table, but typically chat apps just track per-conversation read up to X.

We will ensure all new tables have proper foreign keys into our existing `users` (and `communities` if used). For example, `messages.sender_user_id` references `users.user_id` (note: our `users.user_id` is a text/varchar primary key in the current schema). We must use the same data type for consistency (likely text for user IDs). Prisma or TypeORM can help manage these relations if we use them in the chat service.

**Migration strategy:** We’ll write SQL migrations to create these tables. It’s straightforward since none of the new tables overlap with existing names (we don’t currently have a “messages” or “conversations” table). We might name them with a prefix (e.g., `chat_conversations` etc.) if we want to clearly namespace the feature. But given our existing table naming (we have `posts`, `comments`, etc.), naming them plainly is fine. The migration will also add any necessary indices (e.g., index on `conversation_id` in messages for retrieval speed, index on `user_id` in conversation\_participants for listing a user’s chats).

We should also consider any **data seeding or backfill**: since this is a new feature, we likely start empty. But we might want to import friend relationships as initial conversations (e.g., optionally create a DM conversation for each existing friendship in `user_friends`). That could be done as an on-demand thing (create when a DM is first initiated).

Finally, we need to integrate these with our DB access layer. If using Prisma, we’d add models for Conversation, ConversationParticipant, Message, etc., and generate the client. If using raw SQL (via our existing db utilities), we’ll create repository functions. The **schema is designed to be compatible** with our current schema – it doesn’t alter or break existing tables, just augments. We’ll verify foreign key constraints (e.g., if a user is deleted (though we rarely delete users), we might cascade or set null on their messages’ sender\_id). Likely we’ll mirror how posts/comments handle user deletion (which currently probably set null or cascade – in our `comments` table, `author_user_id` references users with cascade delete, but user deletion is rare).

In summary, the database changes involve creating a normalized chat schema alongside our forum schema. This approach (four tables: conversations, participants, messages, reactions) is a **proven design** seen in many chat systems. It will efficiently support: 1:1 chats (just conversations with two participants), group chats (conversation with many participants), message history queries (join messages with users for names/avatars, etc.), and new features like threads (via self-referencing message replies) and reactions. We will write migrations for this and ensure they run in our migration workflow (adding them to `curia/docs/current-db-schema.md` for documentation as well).

## Recommendations and Next Steps

**Codebase Compatibility:** The above solutions and libraries were chosen to mesh well with our existing architecture. By reusing our authentication and leveraging contexts (e.g., integrating via MessageRouter events), we ensure the chat module feels like a natural extension of Curia. We should plan to fork or copy components from xTeam Chat (for message feeds, channel list UIs, etc.) and integrate them with our contexts (e.g., use our `UserContext` for user data, our Theme/Tailwind config for styling consistency). The open-source UI kits will further speed up development with ready-made chat interface elements.

**Dual-Mode Deployment:** The architecture allows us to deploy the chat service independently if needed (for example, a community could choose to run just the messenger on their site), while also allowing tight integration when used inside Common Ground. We will maintain a **single codebase** for both modes – using runtime checks for iframe mode vs standalone. This is similar to how our `CgLibContext` handles initialization depending on environment. Thorough testing should be done for both use cases (especially the auth flow).

**Database Integration:** We will proceed with adding the chat tables to our PostgreSQL schema. No existing tables are disturbed, and we leverage our current user and community relationships via foreign keys. This means the messenger can immediately use our **wallet identity system** – when a user sends a message, their `user_id` (like an ENS or UP address string) links to the `users` table, so we can display their name and profile picture (as stored in `users.settings` or `user_friends`). The chat service can query the `user_friends` table to list a user’s friends as potential DM recipients (since `user_friends` holds the active friends list, we can allow starting a conversation only with friends if that’s a rule). Real-time friend sync (as done in `UpFriendsService.ts`) means new friendships will become available for chat promptly.

In conclusion, by combining an **open-source chat foundation** (for proven functionality) with our **existing Common Ground platform**, we can implement a Telegram-like messenger relatively quickly. We’ll use the open-source code as a base (ensuring features like threads, reactions, media sharing are covered), integrate a polished UI kit for a familiar chat experience, and architect the backend for reliability (Socket.IO + Redis + Postgres, with optional enhancements like queues). The integration will follow patterns we already use for plugins/iframes, as also exemplified by Rocket.Chat’s embed model. This ensures the messenger feels fully integrated for users in the forum (one-click access, no extra login) while remaining modular.

**Sources:**

* Open-source Next.js chat app with Socket.IO, featuring DMs, group channels, file sharing, reactions, etc.
* Chat UI Kit providing dozens of ready React components for chat UIs (responsive, customizable, themeable).
* Scalable chat architecture using Redis pub/sub and Kafka for async persistence with PostgreSQL.
* Curia integration plan: Sidebar action for “messages” and placeholder for implementing chat interface.
* Rocket.Chat iframe integration (embed chat via iframe with parent SSO and event messaging).
* Recommended chat DB schema: conversations + user\_conversation join + messages table (conv ID, user ID, timestamp, content).
